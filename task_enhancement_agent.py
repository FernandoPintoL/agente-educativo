"""
Task Enhancement Agent
======================

An√°lisis y recomendaciones inteligentes para mejora de tareas/evaluaciones
Utilizando LLM Groq para an√°lisis de contenido y recomendaciones basadas en datos.

Endpoints:
- POST /api/analysis/content-check - Analiza claridad y calidad pedag√≥gica
- POST /api/recommendation/difficulty - Recomienda dificultad basada en datos
- GET /health - Health check
"""

import os
import json
import logging
from typing import Optional, List, Dict, Any
from datetime import datetime

from fastapi import HTTPException
from pydantic import BaseModel, Field
from langchain_groq import ChatGroq
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Configurar logging
logger = logging.getLogger(__name__)

# ============================================================
# MODELOS PYDANTIC
# ============================================================


class Issue(BaseModel):
    """Problema identificado en el contenido"""
    type: str = Field(..., description="Tipo de problema: ambiguity, clarity, grammar, bias, etc")
    severity: str = Field(..., description="Severidad: critical, warning, info")
    text: str = Field(..., description="Texto espec√≠fico problem√°tico")
    explanation: str = Field(..., description="Explicaci√≥n de por qu√© es problema")
    suggestion: str = Field(..., description="Sugerencia para arreglarlo")


class ContentAnalysisRequest(BaseModel):
    """Solicitud de an√°lisis de contenido"""
    content: str = Field(..., description="Instrucci√≥n o pregunta a analizar")
    task_type: str = Field(..., description="Tipo: 'tarea', 'evaluacion', 'pregunta'")
    course_context: Dict[str, str] = Field(
        ...,
        description="Contexto del curso: nombre, nivel, tema"
    )
    student_data: Optional[Dict[str, Any]] = Field(
        None,
        description="Datos de estudiantes: count, avg_score, std_dev"
    )


class ContentAnalysisResponse(BaseModel):
    """Respuesta del an√°lisis de contenido"""
    clarity_score: float = Field(..., description="0-1: qu√© tan claro")
    is_clear: bool = Field(..., description="¬øEs claramente comprensible?")
    issues: List[Issue] = Field(..., description="Problemas identificados")
    bloom_level: str = Field(..., description="Nivel Bloom: remember-understand-apply-analyze-evaluate-create")
    estimated_difficulty: float = Field(..., description="0-1: dificultad estimada")
    estimated_time_minutes: int = Field(..., description="Minutos estimados para completar")
    prerequisites: List[str] = Field(..., description="Conceptos prerequisitos necesarios")
    strengths: List[str] = Field(..., description="Puntos fuertes de la tarea")
    recommendations: List[str] = Field(..., description="Recomendaciones de mejora")


class DifficultyRecommendationRequest(BaseModel):
    """Solicitud de recomendaci√≥n de dificultad"""
    course_id: int
    task_context: Dict[str, Any] = Field(..., description="Contexto de la tarea")
    student_stats: Optional[Dict[str, float]] = Field(
        None,
        description="Estad√≠sticas de estudiantes: avg_score, std_dev"
    )


class DifficultyRecommendation(BaseModel):
    """Recomendaci√≥n de dificultad"""
    recommended_points: int = Field(..., description="Puntos recomendados (0-100)")
    points_range: tuple = Field(..., description="Rango recomendado (min, max)")
    estimated_time_minutes: int = Field(..., description="Tiempo estimado para completar")
    expected_pass_rate: float = Field(..., description="0-1: porcentaje esperado de aprobados")
    difficulty_level: str = Field(..., description="easy, medium, hard")
    reasoning: str = Field(..., description="Explicaci√≥n del razonamiento")


# ============================================================
# MODELOS PARA AN√ÅLISIS DE SOLUCIONES DE ESTUDIANTES
# ============================================================

class StudentSolutionAnalysisRequest(BaseModel):
    """Solicitud de an√°lisis de soluci√≥n de estudiante"""
    task_id: int = Field(..., description="ID de la tarea")
    student_id: int = Field(..., description="ID del estudiante")
    solution_code: str = Field(..., description="C√≥digo de la soluci√≥n a analizar")
    task_type: str = Field(..., description="Tipo: 'tarea', 'evaluacion'")
    language: str = Field(default="auto", description="Lenguaje: python, js, java, etc")


class ConceptoCorreto(BaseModel):
    """Concepto que el estudiante entendi√≥ correctamente"""
    concepto: str
    evidencia: str
    nivel: Optional[str] = "intermedio"


class ConceptoIncompleto(BaseModel):
    """Concepto que est√° incompleto"""
    concepto: str
    que_falta: str
    pregunta_reflexiva: str


class ErrorDetectado(BaseModel):
    """Error o problema en el c√≥digo"""
    tipo: str = Field(..., description="l√≥gica|rendimiento|estilo|seguridad")
    donde: str = Field(..., description="Ubicaci√≥n aproximada")
    problema_descripcion: str
    pregunta_guia: str
    pista: str


class AspectoPorValidar(BaseModel):
    """Aspecto que el estudiante deber√≠a validar"""
    aspecto: str
    pregunta: str
    caso_especial: Optional[str] = None


class StudentSolutionFeedback(BaseModel):
    """Feedback completo para la soluci√≥n"""
    conceptos_correctos: List[ConceptoCorreto]
    conceptos_incompletos: List[ConceptoIncompleto]
    errores_detectados: List[ErrorDetectado]
    aspectos_a_validar: List[AspectoPorValidar]
    pistas_progresivas: List[str]
    cosas_bien_hechas: List[str]
    siguiente_paso: str


class StudentSolutionAnalysisResponse(BaseModel):
    """Respuesta del an√°lisis de soluci√≥n"""
    id: str = Field(default_factory=lambda: str(__import__('uuid').uuid4()))
    analysis_count: int = Field(..., description="N√∫mero de an√°lisis para esta tarea")
    max_analyses: int = Field(default=5)
    feedback: StudentSolutionFeedback
    metadata: Dict[str, Any] = Field(
        default_factory=lambda: {
            "analysis_duration_ms": 0,
            "timestamp": datetime.utcnow().isoformat() + 'Z'
        }
    )


# ============================================================
# MODELOS PARA AN√ÅLISIS DE T√çTULOS
# ============================================================

class TitleAnalysisRequest(BaseModel):
    """Solicitud de an√°lisis de t√≠tulo para tareas/evaluaciones/recursos"""
    titulo: str = Field(..., description="T√≠tulo a analizar")
    content_type: str = Field(..., description="Tipo: 'tarea', 'evaluacion', 'recurso'")
    course_context: Dict[str, Any] = Field(..., description="Contexto del curso")


class TitleAnalysisResponse(BaseModel):
    """Respuesta del an√°lisis de t√≠tulo con sugerencias"""
    success: bool = True
    titulo_original: str
    content_type: str
    descripcion: str = Field(..., description="Descripci√≥n sugerida de la tarea/evaluaci√≥n")
    instrucciones_plantilla: Optional[str] = Field(None, description="Plantilla de instrucciones")
    tiempo_limite: Optional[int] = Field(None, description="Tiempo l√≠mite sugerido en minutos")
    puntuacion_sugerida: Optional[int] = Field(None, description="Puntuaci√≥n sugerida")
    dificultad: str = Field(..., description="easy|medium|hard")
    nivel_bloom: str = Field(..., description="Nivel cognitivo: remember-understand-apply-analyze-evaluate-create")
    observaciones_pedagogicas: List[str] = Field(..., description="Observaciones sobre la pedagog√≠a")
    conceptos: List[str] = Field(..., description="Conceptos clave a ense√±ar/evaluar")
    confidence: float = Field(..., description="0-1: confianza del an√°lisis")
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat() + 'Z')


# ============================================================
# TASK ENHANCEMENT AGENT
# ============================================================


class TaskEnhancementAgent:
    """Agente inteligente para mejora de tareas usando LLM"""

    def __init__(self):
        """Inicializar agente con LLM Groq"""
        api_key = os.getenv('GROQ_API_KEY')
        if not api_key:
            logger.error("GROQ_API_KEY no est√° configurada")
            raise ValueError("GROQ_API_KEY no est√° configurada en .env")

        self.llm = ChatGroq(
            groq_api_key=api_key,
            model_name=os.getenv('GROQ_MODEL', 'llama-3.3-70b-versatile'),
            temperature=float(os.getenv('GROQ_TEMPERATURE', '0.3')),
            max_tokens=2048,
            timeout=30
        )

        logger.info(f"‚úÖ TaskEnhancementAgent inicializado con modelo {self.llm.model_name}")

    def analyze_content_clarity(
        self,
        request: ContentAnalysisRequest
    ) -> ContentAnalysisResponse:
        """
        Analiza claridad y calidad pedag√≥gica del contenido

        Args:
            request: ContentAnalysisRequest con el contenido a analizar

        Returns:
            ContentAnalysisResponse con an√°lisis detallado
        """

        logger.info(f"Analizando contenido tipo {request.task_type}...")

        # Construir prompt detallado
        prompt = f"""Eres un experto educativo analizando la claridad y calidad pedag√≥gica de una tarea.

CONTEXTO DEL CURSO:
- Nombre: {request.course_context.get('nombre', 'Desconocido')}
- Nivel: {request.course_context.get('nivel', 'basico')}
- Tema: {request.course_context.get('tema', 'General')}
{f"- Total estudiantes: {request.student_data.get('count', '?')}" if request.student_data else ""}
{f"- Promedio hist√≥rico: {request.student_data.get('avg_score', '?')}" if request.student_data else ""}

TIPO DE CONTENIDO: {request.task_type}

CONTENIDO A ANALIZAR:
"{request.content}"

AN√ÅLISIS REQUERIDO:

1. CLARIDAD (0-100): ¬øQu√© tan claramente se entiende el contenido?
2. PROBLEMAS: Identifica:
   - Ambig√ºedad: ¬øPalabras que pueden tener m√∫ltiples interpretaciones?
   - Claridad: ¬øFrases confusas o mal estructuradas?
   - Gram√°tica: ¬øErrores gramaticales u ortogr√°ficos?
   - Prejuicio: ¬øContenido discriminatorio o sesgado?
   - Vaguedad: ¬øT√©rminos no definidos claramente?

3. NIVEL BLOOM: ¬øQu√© nivel cognitivo requiere?
   - remember: Recordar hechos b√°sicos
   - understand: Comprender conceptos
   - apply: Aplicar a nuevas situaciones
   - analyze: Desglosar en partes
   - evaluate: Juzgar cr√≠tica mente
   - create: Crear algo nuevo

4. DIFICULTAD (0-1): ¬øCu√°n dif√≠cil es relativamente?
5. TIEMPO: ¬øMinutos estimados para completar?
6. PREREQUISITOS: ¬øQu√© necesitan saber antes?
7. FORTALEZAS: ¬øQu√© est√° bien hecho?
8. RECOMENDACIONES: ¬øC√≥mo mejorar?

RESPONDE EXACTAMENTE CON ESTE JSON (sin markdown):
{{
    "clarity_score": 85,
    "issues": [
        {{
            "type": "ambiguity|clarity|grammar|bias|vagueness",
            "severity": "critical|warning|info",
            "text": "texto espec√≠fico problem√°tico",
            "explanation": "por qu√© es problema",
            "suggestion": "c√≥mo arreglarlo"
        }}
    ],
    "bloom_level": "remember|understand|apply|analyze|evaluate|create",
    "estimated_difficulty": 0.65,
    "estimated_time_minutes": 45,
    "prerequisites": ["concepto1", "concepto2"],
    "strengths": ["fortaleza1", "fortaleza2"],
    "recommendations": ["recomendaci√≥n1", "recomendaci√≥n2"]
}}"""

        try:
            # Llamar a LLM
            response = self.llm.invoke(prompt)
            content = response.content.strip()

            # Limpiar markdown si existe
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            logger.debug(f"Respuesta LLM: {content[:200]}...")

            # Parse JSON
            analysis_data = json.loads(content)

            # Convertir a respuesta tipada
            return ContentAnalysisResponse(
                clarity_score=min(analysis_data.get('clarity_score', 70) / 100, 1.0),
                is_clear=analysis_data.get('clarity_score', 70) >= 75,
                issues=[
                    Issue(**issue)
                    for issue in analysis_data.get('issues', [])
                ],
                bloom_level=analysis_data.get('bloom_level', 'understand'),
                estimated_difficulty=min(analysis_data.get('estimated_difficulty', 0.5), 1.0),
                estimated_time_minutes=max(analysis_data.get('estimated_time_minutes', 30), 5),
                prerequisites=analysis_data.get('prerequisites', []),
                strengths=analysis_data.get('strengths', []),
                recommendations=analysis_data.get('recommendations', [])
            )

        except json.JSONDecodeError as e:
            logger.error(f"Error parseando JSON de LLM: {str(e)}")
            # Retornar an√°lisis por defecto
            return ContentAnalysisResponse(
                clarity_score=0.7,
                is_clear=True,
                issues=[],
                bloom_level="understand",
                estimated_difficulty=0.5,
                estimated_time_minutes=45,
                prerequisites=[],
                strengths=["Contenido bien estructurado"],
                recommendations=["Revisar con expertos del dominio"]
            )

    def recommend_difficulty(
        self,
        course_id: int,
        task_context: Dict[str, Any],
        student_stats: Optional[Dict[str, float]] = None
    ) -> DifficultyRecommendation:
        """
        Recomienda dificultad y puntuaci√≥n basada en contexto y datos hist√≥ricos

        Args:
            course_id: ID del curso
            task_context: Contexto de la tarea (tipo, complejidad, etc)
            student_stats: Estad√≠sticas de estudiantes (promedio, desv. est.)

        Returns:
            DifficultyRecommendation con sugerencias
        """

        logger.info(f"Recomendando dificultad para curso {course_id}...")

        # Valores por defecto si no hay datos
        if not student_stats:
            student_stats = {
                'avg_score': 75,
                'std_dev': 15,
                'count': 0
            }

        task_complexity = task_context.get('complexity', 0.5)
        task_type = task_context.get('type', 'tarea')

        # Construcci√≥n del prompt
        prompt = f"""Eres un experto en dise√±o curricular y evaluaci√≥n educativa.
Tu tarea es recomendar la dificultad y puntuaci√≥n para una nueva tarea.

CONTEXTO DE LA TAREA:
- Tipo: {task_type}
- Complejidad estimada: {task_complexity}/1.0 (0=trivial, 1=muy dif√≠cil)
- T√≠tulo: {task_context.get('title', 'Sin t√≠tulo')}

DATOS HIST√ìRICOS DEL CURSO:
- Promedio estudiantes: {student_stats.get('avg_score', 75):.1f} puntos
- Desv. est√°ndar: {student_stats.get('std_dev', 15):.1f}
- Total estudiantes: {student_stats.get('count', 0)}

OBJETIVO PEDAG√ìGICO:
- 60-70% de estudiantes deber√≠an PASAR (nota ‚â• 60)
- 20-30% deber√≠an obtener BUENA nota (‚â• 80)
- 10% EXCELENTE (‚â• 95)

Esto significa una distribuci√≥n aproximadamente normal con media ‚âà 75.

CONSIDERACIONES:
- Si complejidad es baja: m√°s puntos base, tarea m√°s f√°cil
- Si complejidad es alta: menos puntos base, pero m√°s desafiante
- Rango recomendado para puntuaci√≥n: 50-100

DEVUELVE EXACTAMENTE ESTE JSON (sin markdown):
{{
    "recommended_points": 85,
    "points_range": [70, 95],
    "estimated_time_minutes": 60,
    "expected_pass_rate": 0.67,
    "difficulty_level": "medium",
    "reasoning": "Basado en el promedio hist√≥rico del curso ({student_stats.get('avg_score', 75):.0f}), recomiendo..."
}}"""

        try:
            response = self.llm.invoke(prompt)
            content = response.content.strip()

            # Limpiar markdown si existe
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            logger.debug(f"Respuesta LLM dificultad: {content[:200]}...")

            # Parse JSON
            data = json.loads(content)

            points_range = tuple(data.get('points_range', [70, 90]))

            return DifficultyRecommendation(
                recommended_points=int(data.get('recommended_points', 85)),
                points_range=points_range,
                estimated_time_minutes=int(data.get('estimated_time_minutes', 60)),
                expected_pass_rate=float(data.get('expected_pass_rate', 0.65)),
                difficulty_level=data.get('difficulty_level', 'medium'),
                reasoning=data.get('reasoning', 'Recomendaci√≥n basada en contexto del curso')
            )

        except (json.JSONDecodeError, KeyError) as e:
            logger.error(f"Error en recomendaci√≥n de dificultad: {str(e)}")
            # Retornar recomendaci√≥n por defecto
            avg_score = student_stats.get('avg_score', 75)
            return DifficultyRecommendation(
                recommended_points=int(avg_score * 0.9),
                points_range=(int(avg_score * 0.75), int(avg_score * 1.1)),
                estimated_time_minutes=60,
                expected_pass_rate=0.65,
                difficulty_level='medium',
                reasoning=f"Basado en promedio hist√≥rico de {avg_score:.0f}"
            )

    def analyze_student_solution(
        self,
        request: StudentSolutionAnalysisRequest
    ) -> StudentSolutionAnalysisResponse:
        """
        Analiza la soluci√≥n de un estudiante usando m√©todo socr√°tico.

        IMPORTANTE: NUNCA da la soluci√≥n directa, solo gu√≠a con preguntas.

        Args:
            request: Datos de la soluci√≥n a analizar

        Returns:
            Feedback sin respuestas directas
        """
        import time
        start_time = time.time()

        try:
            # Validar entrada
            if not request.solution_code or len(request.solution_code.strip()) < 10:
                raise ValueError("El c√≥digo debe tener al menos 10 caracteres")

            # Construir el prompt (M√âTODO SOCR√ÅTICO)
            prompt = self._build_student_feedback_prompt(request)

            logger.info(f"Analizando soluci√≥n de estudiante {request.student_id}...")

            # Llamar LLM
            response = self.llm.invoke(prompt)

            # Procesar respuesta
            content = response.content.strip()

            # Limpiar markdown si est√° presente
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            # Parse JSON
            feedback_data = json.loads(content)

            # Construir respuesta tipada
            duration_ms = int((time.time() - start_time) * 1000)

            return StudentSolutionAnalysisResponse(
                analysis_count=1,  # Este n√∫mero lo maneja el backend
                max_analyses=5,
                feedback=StudentSolutionFeedback(
                    conceptos_correctos=[
                        ConceptoCorreto(**c) for c in feedback_data.get('conceptos_correctos', [])
                    ],
                    conceptos_incompletos=[
                        ConceptoIncompleto(**c) for c in feedback_data.get('conceptos_incompletos', [])
                    ],
                    errores_detectados=[
                        ErrorDetectado(**e) for e in feedback_data.get('errores_detectados', [])
                    ],
                    aspectos_a_validar=[
                        AspectoPorValidar(**a) for a in feedback_data.get('aspectos_a_validar', [])
                    ],
                    pistas_progresivas=feedback_data.get('pistas_progresivas', []),
                    cosas_bien_hechas=feedback_data.get('cosas_bien_hechas', []),
                    siguiente_paso=feedback_data.get('siguiente_paso', 'Contin√∫a iterando')
                ),
                metadata={
                    'analysis_duration_ms': duration_ms,
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'llm_model_used': 'llama-3.3-70b-versatile',
                    'task_type': request.task_type,
                    'language': request.language
                }
            )

        except json.JSONDecodeError as e:
            logger.error(f"Error parseando respuesta LLM: {str(e)}")
            # Retornar feedback gen√©rico
            return StudentSolutionAnalysisResponse(
                analysis_count=1,
                feedback=StudentSolutionFeedback(
                    conceptos_correctos=[ConceptoCorreto(
                        concepto="Estructura b√°sica",
                        evidencia="Detectamos c√≥digo v√°lido",
                        nivel="b√°sico"
                    )],
                    conceptos_incompletos=[],
                    errores_detectados=[],
                    aspectos_a_validar=[AspectoPorValidar(
                        aspecto="Validaci√≥n",
                        pregunta="¬øValidaste todos los casos especiales?"
                    )],
                    pistas_progresivas=[
                        "üí° Piensa en casos especiales",
                        "üí° ¬øQu√© pasa con entrada negativa?",
                        "üí° ¬øY con entrada vac√≠a?",
                    ],
                    cosas_bien_hechas=["C√≥digo bien formateado"],
                    siguiente_paso="Revisa con un compa√±ero"
                )
            )

        except Exception as e:
            logger.error(f"Error analizando soluci√≥n: {str(e)}")
            raise

    def analyze_task_title(
        self,
        request: TitleAnalysisRequest
    ) -> TitleAnalysisResponse:
        """
        Analiza un t√≠tulo de tarea y genera sugerencias para completar la tarea

        Args:
            request: TitleAnalysisRequest con el t√≠tulo y contexto

        Returns:
            TitleAnalysisResponse con sugerencias
        """
        logger.info(f"Analizando t√≠tulo de tarea: {request.titulo[:50]}...")

        return self._analyze_title(request, 'tarea')

    def analyze_evaluation_title(
        self,
        request: TitleAnalysisRequest
    ) -> TitleAnalysisResponse:
        """
        Analiza un t√≠tulo de evaluaci√≥n y genera sugerencias

        Args:
            request: TitleAnalysisRequest con el t√≠tulo y contexto

        Returns:
            TitleAnalysisResponse con sugerencias
        """
        logger.info(f"Analizando t√≠tulo de evaluaci√≥n: {request.titulo[:50]}...")

        return self._analyze_title(request, 'evaluacion')

    def analyze_resource_title(
        self,
        request: TitleAnalysisRequest
    ) -> TitleAnalysisResponse:
        """
        Analiza un t√≠tulo de recurso y genera sugerencias

        Args:
            request: TitleAnalysisRequest con el t√≠tulo y contexto

        Returns:
            TitleAnalysisResponse con sugerencias
        """
        logger.info(f"Analizando t√≠tulo de recurso: {request.titulo[:50]}...")

        return self._analyze_title(request, 'recurso')

    def _analyze_title(
        self,
        request: TitleAnalysisRequest,
        content_type: str
    ) -> TitleAnalysisResponse:
        """
        An√°lisis gen√©rico de t√≠tulo que se reutiliza para diferentes tipos de contenido

        Args:
            request: TitleAnalysisRequest con el t√≠tulo y contexto
            content_type: 'tarea', 'evaluacion', o 'recurso'

        Returns:
            TitleAnalysisResponse con an√°lisis y sugerencias
        """
        try:
            # Construir prompt espec√≠fico para el tipo de contenido
            prompt = self._build_title_analysis_prompt(request, content_type)

            # Llamar a LLM
            response = self.llm.invoke(prompt)
            content = response.content.strip()

            # Limpiar markdown si existe
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()

            logger.debug(f"Respuesta LLM an√°lisis de t√≠tulo: {content[:200]}...")

            # Parse JSON
            analysis_data = json.loads(content)

            # Convertir a respuesta tipada
            return TitleAnalysisResponse(
                success=True,
                titulo_original=request.titulo,
                content_type=content_type,
                descripcion=analysis_data.get('descripcion', ''),
                instrucciones_plantilla=analysis_data.get('instrucciones_plantilla'),
                tiempo_limite=analysis_data.get('tiempo_limite'),
                puntuacion_sugerida=analysis_data.get('puntuacion_sugerida'),
                dificultad=analysis_data.get('dificultad', 'medium'),
                nivel_bloom=analysis_data.get('nivel_bloom', 'understand'),
                observaciones_pedagogicas=analysis_data.get('observaciones_pedagogicas', []),
                conceptos=analysis_data.get('conceptos', []),
                confidence=analysis_data.get('confidence', 0.75)
            )

        except json.JSONDecodeError as e:
            logger.error(f"Error parseando JSON de an√°lisis de t√≠tulo: {str(e)}")
            # Retornar respuesta por defecto
            return TitleAnalysisResponse(
                success=True,
                titulo_original=request.titulo,
                content_type=content_type,
                descripcion=f"Tarea basada en: {request.titulo}",
                dificultad='medium',
                nivel_bloom='understand',
                observaciones_pedagogicas=['Revisar con expertos del dominio'],
                conceptos=['Conceptos principales del t√≠tulo'],
                confidence=0.5
            )

        except Exception as e:
            logger.error(f"Error analizando t√≠tulo: {str(e)}")
            raise

    def _build_title_analysis_prompt(
        self,
        request: TitleAnalysisRequest,
        content_type: str
    ) -> str:
        """
        Construye el prompt para an√°lisis de t√≠tulo

        Args:
            request: TitleAnalysisRequest
            content_type: 'tarea', 'evaluacion', o 'recurso'

        Returns:
            Prompt para LLM
        """

        tipo_content = {
            'tarea': 'tarea educativa',
            'evaluacion': 'evaluaci√≥n o examen',
            'recurso': 'recurso de aprendizaje'
        }.get(content_type, 'contenido educativo')

        curso_nombre = request.course_context.get('nombre', 'Sin especificar')
        curso_nivel = request.course_context.get('nivel', 'intermedio')

        return f"""Eres un experto en dise√±o pedag√≥gico y educaci√≥n.
Tu tarea es analizar el siguiente t√≠tulo de {tipo_content} y generar sugerencias completas para crear contenido educativo de calidad.

CONTEXTO DEL CURSO:
- Nombre: {curso_nombre}
- Nivel: {curso_nivel}

T√çTULO A ANALIZAR:
"{request.titulo}"

TIPO DE CONTENIDO: {content_type}

Tu tarea es generar:

1. **DESCRIPCI√ìN**: Una descripci√≥n clara y pedag√≥gica del {tipo_content}
2. **INSTRUCCIONES (si aplica)**: Una plantilla de instrucciones detalladas
3. **TIEMPO L√çMITE**: Tiempo estimado en minutos para completar
4. **PUNTUACI√ìN**: Puntuaci√≥n sugerida (100 m√°ximo)
5. **DIFICULTAD**: easy, medium, o hard
6. **NIVEL BLOOM**: Nivel cognitivo de Bloom (remember, understand, apply, analyze, evaluate, create)
7. **OBSERVACIONES PEDAG√ìGICAS**: 3-5 observaciones sobre c√≥mo ense√±ar/evaluar esto
8. **CONCEPTOS CLAVE**: 3-5 conceptos principales a ense√±ar/evaluar
9. **CONFIANZA**: 0-1, qu√© tan confiable es este an√°lisis (0.7-0.9 t√≠picamente)

RESPONDE EXACTAMENTE CON ESTE JSON (sin markdown):
{{
    "descripcion": "Descripci√≥n clara y detallada de la {tipo_content}",
    "instrucciones_plantilla": "Plantilla de instrucciones paso a paso (si aplica)",
    "tiempo_limite": 45,
    "puntuacion_sugerida": 100,
    "dificultad": "medium",
    "nivel_bloom": "apply",
    "observaciones_pedagogicas": [
        "Observaci√≥n pedag√≥gica 1",
        "Observaci√≥n pedag√≥gica 2",
        "Observaci√≥n pedag√≥gica 3"
    ],
    "conceptos": [
        "Concepto clave 1",
        "Concepto clave 2",
        "Concepto clave 3"
    ],
    "confidence": 0.85
}}

IMPORTANTE:
- S√© espec√≠fico y detallado
- Crea contenido pedag√≥gicamente s√≥lido
- Usa lenguaje educativo
- Sugiere actividades que promuevan el aprendizaje activo
- La descripci√≥n debe ser clara para estudiantes de nivel {curso_nivel}
"""

    def _build_student_feedback_prompt(
        self,
        request: StudentSolutionAnalysisRequest
    ) -> str:
        """
        Construye el prompt para an√°lisis de soluci√≥n usando M√âTODO SOCR√ÅTICO.

        CR√çTICO: El prompt DEBE:
        - NO resolver el problema
        - NO dar c√≥digo compilable
        - S√ç hacer preguntas reflexivas
        - S√ç se√±alar problemas
        - S√ç guiar sin responder
        """

        return f"""
Eres un profesor de programaci√≥n usando el M√©todo Socr√°tico.
Tu objetivo: hacer que el estudiante PIENSE, no darle respuestas directas.

CONTEXTO:
- Tipo de tarea: {request.task_type}
- Lenguaje: {request.language}
- Estudiante ID: {request.student_id}

C√ìDIGO DEL ESTUDIANTE:
```{request.language if request.language != 'auto' else 'code'}
{request.solution_code}
```

AN√ÅLISIS REQUERIDO (formato JSON):

Tu tarea es analizar este c√≥digo y proporcionar feedback usando el M√©todo Socr√°tico.

REGLAS ESTRICTAS:
1. NUNCA escribas c√≥digo compilable que resuelva el problema
2. NUNCA des instrucciones paso a paso
3. NUNCA especifiques "cambia X por Y"
4. NUNCA hagas la tarea por el estudiante
5. SIEMPRE haz preguntas reflexivas
6. SIEMPRE s√© constructivo y alentador

ESTRUCTURA DE RESPUESTA (JSON v√°lido):

{{
  "conceptos_correctos": [
    {{
      "concepto": "nombre del concepto bien entendido",
      "evidencia": "prueba de d√≥nde en el c√≥digo lo vemos",
      "nivel": "b√°sico|intermedio|avanzado"
    }}
  ],

  "conceptos_incompletos": [
    {{
      "concepto": "nombre del concepto",
      "que_falta": "qu√© parte falta o est√° incompleta",
      "pregunta_reflexiva": "¬øPregunta que lo haga pensar?"
    }}
  ],

  "errores_detectados": [
    {{
      "tipo": "l√≥gica|rendimiento|estilo|seguridad",
      "donde": "ubicaci√≥n aproximada (l√≠nea X-Y o secci√≥n)",
      "problema_descripcion": "descripci√≥n del problema SIN c√≥mo resolverlo",
      "pregunta_guia": "pregunta reflexiva sobre el problema",
      "pista": "üí° Una pista sin resolver"
    }}
  ],

  "aspectos_a_validar": [
    {{
      "aspecto": "validaci√≥n|documentaci√≥n|robustez|etc",
      "pregunta": "¬øPregunta reflexiva sobre este aspecto?",
      "caso_especial": "Ejemplo: ¬øy si el input est√° vac√≠o?"
    }}
  ],

  "pistas_progresivas": [
    "üí° Pista 1 (general, abierta)",
    "üí° Pista 2 (m√°s espec√≠fica)",
    "üí° Pista 3 (a√∫n m√°s espec√≠fica)",
    "üí° Pista 4 (casi un hint, pero no la soluci√≥n)",
    "üí° Pista 5 (reflexi√≥n final)"
  ],

  "cosas_bien_hechas": [
    "Lo que el estudiante hizo bien",
    "Otro punto positivo",
    "Etc"
  ],

  "siguiente_paso": "Una sola acci√≥n enfocada para el estudiante"
}}

EJEMPLOS DE QU√â HACER / QU√â NO HACER:

‚ùå MALO:
"Tu funci√≥n est√° mal. Usa un bucle for como esto: for i in range(n):"

‚úÖ BUENO:
"Tu funci√≥n no maneja casos especiales. ¬øQu√© sucede si alguien pasa un n√∫mero negativo?"

‚ùå MALO:
"Deber√≠as agregar validaci√≥n as√≠: if n < 0: raise ValueError(...)"

‚úÖ BUENO:
"¬øC√≥mo maneja tu c√≥digo entrada inv√°lida? ¬øQu√© deber√≠a pasar?"

TONO Y ESTILO:
- Profesional pero amable
- Desafiante pero no frustrante
- Educativo, nunca condescendiente
- Usa ejemplos del c√≥digo EXISTENTE del estudiante
- Gu√≠a, nunca resuelves

Ahora analiza el c√≥digo proporcionado y responde SOLO en formato JSON v√°lido.
No incluyas texto adicional, solo el JSON.
"""


# ============================================================
# INSTANCIA GLOBAL
# ============================================================

try:
    agent = TaskEnhancementAgent()
except Exception as e:
    logger.error(f"Error inicializando TaskEnhancementAgent: {str(e)}")
    agent = None
